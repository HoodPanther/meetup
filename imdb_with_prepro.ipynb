{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial 2: Sentiment analysis with an recurrent LSTM network\n",
    "=============================================================\n",
    "\n",
    "Train an recurrent neural network to parse movie reviews from IMDB and decide if they are positive or negative reviews.\n",
    "\n",
    "An example of a review:\n",
    "\n",
    "`\"Okay, sorry, but I loved this movie. I just love the whole 80's genre of these kind of movies, because you don't see many like this one anymore! I want to ask all of you people who say this movie is just a rip-off, or a cheesy imitation, what is it imitating? I've never seen another movie like this one, well, not horror anyway.\n",
    "Basically its about the popular group in school, who like to make everyones lives living hell, so they decided to pick on this nerdy boy named Marty. It turns fatal when he really gets hurt from one of their little pranks.\n",
    "So, its like 10 years later, and the group of friends who hurt Marty start getting High School reunion letters. But...they are the only ones receiving them! So they return back to the old school, and one by one get knocked off by.......Yeah you probably know what happens!\n",
    "The only part that disappointed me was the very end. It could have been left off, or thought out better.\n",
    "I think you should give it a try, and try not to be to critical!\n",
    "~*~CupidGrl~*~\"`\n",
    "\n",
    "We have 25000 reviews like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data preparation\n",
    "-------------------\n",
    "We have a script that takes reviews from a text file and store as one-hot encoded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size -  81321\n",
      "# of samples -  25000\n",
      "# of classes 2\n",
      "class distribution -  [0 1] [12500 12500]\n",
      "sentence length -  1108 [  10   11   12 ..., 1919 1977 2633] [1 1 1 ..., 1 1 1]\n",
      "# of train - 19881, # of valid - 5119\n"
     ]
    }
   ],
   "source": [
    "# read reviews from text file and store as one-hot encoded dataset\n",
    "import prepare\n",
    "fname='labeledTrainData.tsv'\n",
    "prepare.main(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Building the model\n",
    "---------------------\n",
    "Similar to the convnet example, we need dataset, layers, callbacks, backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 128\n",
    "embedding_dim = 128\n",
    "vocab_size = 20000\n",
    "sentence_length = 128\n",
    "batch_size = 128\n",
    "num_epochs = 2\n",
    "\n",
    "# setup backend\n",
    "from neon.backends import gen_backend\n",
    "be = gen_backend(backend='cpu',\n",
    "                 batch_size=batch_size,\n",
    "                 rng_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train examples - 19881, valid examples - 5119\n",
      "# of classes -  2\n",
      "class distribution -  [12500 12500]\n",
      "vocab size - 20000, sentence_length - 128\n"
     ]
    }
   ],
   "source": [
    "# load the h5 datasets, print stats\n",
    "import h5py\n",
    "h5f = h5py.File(fname + '.h5', 'r')\n",
    "reviews, h5train, h5valid = h5f['reviews'], h5f['train'], h5f['valid']\n",
    "ntrain, nvalid, nclass = reviews.attrs['ntrain'], reviews.attrs['nvalid'], reviews.attrs['nclass']\n",
    "print \"# of train examples - {0}, valid examples - {1}\".format(ntrain, nvalid)\n",
    "print \"# of classes - \", nclass\n",
    "print \"class distribution - \", reviews.attrs['class_distribution']\n",
    "print \"vocab size - {0}, sentence_length - {1}\".format(vocab_size, sentence_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datsets\n",
    "Extract a training and validation set from the raw dataset and pad / truncate reviews to 128 words. Finally wrap them into a DataIterator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make train dataset\n",
    "from preprocess_text import get_paddedXY\n",
    "from neon.data import DataIterator\n",
    "Xy = h5train[:ntrain]\n",
    "X = [xy[1:] for xy in Xy]\n",
    "y = [xy[0] for xy in Xy]\n",
    "X_train, y_train = get_paddedXY(\n",
    "    X, y, vocab_size=vocab_size, sentence_length=sentence_length)\n",
    "train_set = DataIterator(X_train, y_train, nclass=nclass)\n",
    "\n",
    "# make valid dataset\n",
    "Xy = h5valid[:nvalid]\n",
    "X = [xy[1:] for xy in Xy]\n",
    "y = [xy[0] for xy in Xy]\n",
    "X_valid, y_valid = get_paddedXY(\n",
    "    X, y, vocab_size=vocab_size, sentence_length=sentence_length)\n",
    "valid_set = DataIterator(X_valid, y_valid, nclass=nclass)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intializers\n",
    "\n",
    "We use \"Glorot Initialization\" to automatically scale the weights to preserve the variance of input activations on the output side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialization\n",
    "from neon.initializers import GlorotUniform, Uniform\n",
    "init_glorot = GlorotUniform()\n",
    "init_emb = Uniform(-0.1 / embedding_dim, 0.1 / embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model layers\n",
    "* The network consists of a word embedding layer, and LSTM, a RecurrentSum and and Affine layer.\n",
    "* LookupTable is a word embedding that maps from a sparse one-hot representation to dense wordvectors. The embedding is learned from the data\n",
    "* LSTM is a recurrent layer with \"long short-term memory\" units. LSTM networks tend to be easier to train, but generally perform similar to standard RNN layers\n",
    "* RecurrentSum is a recurrent output layer that collapeses over the time dimension of the sequence by summing up outputs from individual steps.\n",
    "* Dropout performs regularizaion by randomly zeroing out some of the units\n",
    "* Affine is a fully connected MLP layer that is used for the binary classification of the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define layers\n",
    "from neon.layers import LookupTable, LSTM, RecurrentSum, Dropout, Affine\n",
    "from neon.transforms import Softmax, Tanh, Logistic\n",
    "layers = [\n",
    "    LookupTable(vocab_size=vocab_size, embedding_dim=embedding_dim, init=init_emb),\n",
    "    LSTM(hidden_size, init_glorot, activation=Tanh(),\n",
    "         gate_activation=Logistic(), reset_cells=True),\n",
    "    RecurrentSum(),\n",
    "    Dropout(keep=0.5),\n",
    "    Affine(nclass, init_glorot, bias=init_glorot, activation=Softmax())\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set the cost, metrics, optimizer\n",
    "from neon.layers import GeneralizedCost\n",
    "from neon.transforms import CrossEntropyMulti\n",
    "from neon.transforms import Accuracy\n",
    "from neon.models import Model\n",
    "from neon.optimizers import Adagrad\n",
    "cost = GeneralizedCost(costfunc=CrossEntropyMulti(usebits=True))\n",
    "metric = Accuracy()\n",
    "model = Model(layers=layers)\n",
    "optimizer = Adagrad(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks\n",
    "In addition to the default progress bar, we set up a callback to save the model to a pickle file after every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# configure callbacks\n",
    "from neon.callbacks import Callbacks\n",
    "callbacks = Callbacks(model, train_set, eval_set=valid_set, \n",
    "                      epochs=num_epochs, serialize=1,\n",
    "                      save_path=fname + '.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "We now have all the parts in place to train the model. Two epochs are sufficient to obtain some interesting results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0   [Train |████████████████████|  156/156  batches, 0.41 cost, 108.16s]\n",
      "Epoch 1   [Train |████████████████████|  155/155  batches, 0.21 cost, 106.63s]\n",
      "\n",
      "Test Accuracy - [ 86.38405609]\n",
      "Train Accuracy - [ 95.5736618]\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model.fit(train_set, optimizer=optimizer, num_epochs=num_epochs,\n",
    "          cost=cost, callbacks=callbacks)\n",
    "\n",
    "# eval model\n",
    "print \"\\nTest Accuracy -\", 100 * model.eval(valid_set, metric=metric)\n",
    "print \"Train Accuracy -\", 100 * model.eval(train_set, metric=metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "3. TODO: Inference\n",
    "------------\n",
    "The trained model can now be used to perform inference on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hyperparameters from the reference\n",
    "batch_size = 1\n",
    "clip_gradients = True\n",
    "gradient_limit = 15\n",
    "vocab_size = 20000\n",
    "sentence_length = 128\n",
    "embedding_dim = 128\n",
    "hidden_size = 128\n",
    "reset_cells = True\n",
    "#fname='labeledTrainData.tsv'\n",
    "save_path= 'labeledTrainData.tsv' + '.pickle'\n",
    "#num_epochs = args.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setup backend\n",
    "from neon.backends import gen_backend\n",
    "be = gen_backend(#backend=args.backend,\n",
    "                 batch_size=batch_size,\n",
    "                 #rng_seed=args.rng_seed,\n",
    "                 #device_id=args.device_id,\n",
    "                 #default_dtype=args.datatype\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neon.initializers import GlorotUniform, Uniform\n",
    "init_glorot = GlorotUniform()\n",
    "init_emb = Uniform(-0.1 / embedding_dim, 0.1 / embedding_dim)\n",
    "nclass = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define same model as in train\n",
    "from neon.layers import LookupTable, LSTM, RecurrentSum, Dropout, Affine\n",
    "from neon.transforms import Tanh, Softmax, Logistic\n",
    "layers = [\n",
    "\n",
    "    LookupTable(vocab_size=vocab_size, embedding_dim=embedding_dim, init=init_emb),\n",
    "    LSTM(hidden_size, init_glorot, activation=Tanh(),\n",
    "         gate_activation=Logistic(), reset_cells=True),\n",
    "    RecurrentSum(),\n",
    "    Dropout(keep=0.5),\n",
    "    Affine(nclass, init_glorot, bias=init_glorot, activation=Softmax())\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized the models - \n",
      "Loading the weights from labeledTrainData.tsv.pickle\n"
     ]
    }
   ],
   "source": [
    "# load the weights\n",
    "from neon.models import Model\n",
    "print \"Initialized the models - \"\n",
    "model_new = Model(layers=layers)\n",
    "print \"Loading the weights from {0}\".format(save_path)\n",
    "\n",
    "model_new.load_weights(save_path)\n",
    "model_new.initialize(dataset=(sentence_length, batch_size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setup buffers before accepting reviews\n",
    "import cPickle\n",
    "import numpy as np\n",
    "xdev = be.zeros((sentence_length, 1), dtype=np.int32)  # bsz is 1, feature size\n",
    "xbuf = np.zeros((1, sentence_length), dtype=np.int32)\n",
    "oov = 2\n",
    "start = 1\n",
    "index_from = 3\n",
    "pad_char = 0\n",
    "vocab, rev_vocab = cPickle.load(open(fname + '.vocab', 'rb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a Review from testData.tsv file \n",
      "preprocess_text\n",
      "Sent - [[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    1\n",
      "     2 3004]]\n",
      "Pred - [[ 0.30626768  0.69373232]] \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Enter a Review from testData.tsv file \n",
      "The only part that disappointed me was the very end. It could have been left off, or thought out better. I think you should give it a try, and try not to be to critical!\n",
      "Sent - [[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    1    3   71  176   15  678   79   17    3\n",
      "    62  136   11   85   33   87  319  130    4   48  201   53  134   13\n",
      "   110   27  141  206   11    6  354    4    5  354   29    8   34    8\n",
      "  2778   36]]\n",
      "Pred - [[ 0.78164196  0.2183581 ]] \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Enter a Review from testData.tsv file \n",
      "I love every minute\n",
      "Sent - [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1  13 124\n",
      "  179 777]]\n",
      "Pred - [[ 0.24156739  0.75843257]] \n",
      "----------------------------------------------------------------------------------------------------\n",
      "Enter a Review from testData.tsv file \n",
      "I hate the movie\n",
      "Sent - [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1  13 780\n",
      "    3  21]]\n",
      "Pred - [[ 0.33871099  0.66128898]] \n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import preprocess_text\n",
    "while True:\n",
    "    line = raw_input('Enter a Review from testData.tsv file \\n')\n",
    "\n",
    "    # clean the input\n",
    "    tokens = preprocess_text.clean_string(line).strip().split()\n",
    "\n",
    "    # check for oov and add start\n",
    "    sent = [len(vocab) + 1 if t not in vocab else vocab[t] for t in tokens]\n",
    "    sent = [start] + [w + index_from for w in sent]\n",
    "    sent = [oov if w >= vocab_size else w for w in sent]\n",
    "\n",
    "    # pad sentences\n",
    "    xbuf[:] = 0\n",
    "    trunc = sent[-sentence_length:]\n",
    "    xbuf[0, -len(trunc):] = trunc\n",
    "    xdev[:] = xbuf.T.copy()\n",
    "    y_pred = model_new.fprop(xdev, inference=True)  # inference flag dropout\n",
    "\n",
    "    print \"Sent - {0}\".format(xbuf)\n",
    "    print \"Pred - {0} \".format(y_pred.get().T)\n",
    "    print '-' * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "asdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
