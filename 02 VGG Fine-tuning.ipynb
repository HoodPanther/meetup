{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Fine-tuning VGG on CIFAR-10\n",
    "\n",
    "One of the most common questions we get is how to use neon to load a pre-trained model and fine-tune on a new dataset. In this tutorial, we show how to load a pre-trained convolutional neural network (VGG), which was trained on ImageNet, a large corpus of natural images with 1000 categories. We will then use this model to train on the CIFAR-10 dataset, a much smaller set of images with 10 categories.\n",
    "\n",
    "We begin by first generating a computational backend with the `gen_backend` function from neon. If there is a GPU available (recommended), this function will generate a GPU backend. Otherwise, a CPU backend will be used.\n",
    "\n",
    "**Note: this ipython notebook will run with a CPU backend, but the training portion will take too long (~2 days for 1 epoch) to see results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neon.backends import gen_backend\n",
    "\n",
    "be = gen_backend(batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the backend via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the VGG Model\n",
    "We begin by first generating our VGG network. [VGG](http://www.robots.ox.ac.uk/~vgg/research/very_deep/) is a popular convolutional neural network with ~19 layers. Not only does this network perform well with fine-tuning, but is also easy to define since the convolutional layers all have 3x3 filter sizes, and only differ in the number of feature maps. \n",
    "\n",
    "We first define some common parameters used by all the convolution layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neon.transforms import Rectlin\n",
    "from neon.initializers import Constant, Xavier\n",
    "\n",
    "relu = Rectlin()\n",
    "conv_params = {'strides': 1,\n",
    "               'padding': 1,\n",
    "               'init': Xavier(local=True),\n",
    "               'bias': Constant(0),\n",
    "               'activation': relu}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can define our network as a list of layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neon.layers import Conv, Dropout, Pooling, GeneralizedCost, Affine\n",
    "from neon.initializers import GlorotUniform\n",
    "\n",
    "\n",
    "# Set up the model layers\n",
    "vgg_layers = []\n",
    "\n",
    "# set up 3x3 conv stacks with different number of filters\n",
    "vgg_layers.append(Conv((3, 3, 64), **conv_params))\n",
    "vgg_layers.append(Conv((3, 3, 64), **conv_params))\n",
    "vgg_layers.append(Pooling(2, strides=2))\n",
    "vgg_layers.append(Conv((3, 3, 128), **conv_params))\n",
    "vgg_layers.append(Conv((3, 3, 128), **conv_params))\n",
    "vgg_layers.append(Pooling(2, strides=2))\n",
    "vgg_layers.append(Conv((3, 3, 256), **conv_params))\n",
    "vgg_layers.append(Conv((3, 3, 256), **conv_params))\n",
    "vgg_layers.append(Conv((3, 3, 256), **conv_params))\n",
    "vgg_layers.append(Pooling(2, strides=2))\n",
    "vgg_layers.append(Conv((3, 3, 512), **conv_params))\n",
    "vgg_layers.append(Conv((3, 3, 512), **conv_params))\n",
    "vgg_layers.append(Conv((3, 3, 512), **conv_params))\n",
    "vgg_layers.append(Pooling(2, strides=2))\n",
    "vgg_layers.append(Conv((3, 3, 512), **conv_params))\n",
    "vgg_layers.append(Conv((3, 3, 512), **conv_params))\n",
    "vgg_layers.append(Conv((3, 3, 512), **conv_params))\n",
    "vgg_layers.append(Pooling(2, strides=2))\n",
    "vgg_layers.append(Affine(nout=4096, init=GlorotUniform(), bias=Constant(0), activation=relu))\n",
    "vgg_layers.append(Dropout(keep=0.5))\n",
    "vgg_layers.append(Affine(nout=4096, init=GlorotUniform(), bias=Constant(0), activation=relu))\n",
    "vgg_layers.append(Dropout(keep=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last layer of VGG is an `Affine` layer with 1000 units, for the 1000 categories in the ImageNet dataset. However, since our dataset only has 10 classes, we will instead use 10 output units. We also give this layer a special name (`class_layer`) so we know not to load pre-trained weights for this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neon.transforms import Softmax\n",
    "\n",
    "vgg_layers.append(Affine(nout=10, init=GlorotUniform(), bias=Constant(0), activation=Softmax(),\n",
    "                  name=\"class_layer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to load the pre-trained weights into this model. First we generate a `Model` object to hold the VGG layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neon.models import Model\n",
    "\n",
    "model = Model(layers=vgg_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pre-trained weights\n",
    "Next, we download the pre-trained VGG weights from our [Model Zoo](https://github.com/nervanazoo/NervanaModelZoo/tree/master/ImageClassification/ILSVRC2012/VGG). Note: this file is quite large (~550MB). By default, the weights file is saved in your home directory. To change this, or if you have already downloaded the file somewhere else, please edit the `filepath` variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neon.data.datasets import Dataset\n",
    "from neon.util.persist import load_obj\n",
    "import os\n",
    "\n",
    "# location and size of the VGG weights file\n",
    "url = 'https://s3-us-west-1.amazonaws.com/nervana-modelzoo/VGG/'\n",
    "filename = 'VGG_D.p'\n",
    "size = 554227541\n",
    "\n",
    "# edit filepath below if you have the file elsewhere\n",
    "_, filepath = Dataset._valid_path_append('data', '', filename)\n",
    "if not os.path.exists(filepath):\n",
    "    Dataset.fetch_dataset(url, filename, filepath, size)\n",
    "\n",
    "# load the weights param file\n",
    "print(\"Loading VGG weights from {}...\".format(filepath))\n",
    "trained_vgg = load_obj(filepath)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In neon, models are saved as python dictionaries. Below are some example calls to explore the model. You can examine the weights, the layer configuration, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"The dictionary has the following keys: {}\".format(trained_vgg.keys()))\n",
    "\n",
    "layer0 = trained_vgg['model']['config']['layers'][0]\n",
    "print(\"The first layer is of type: {}\".format(layer0['type']))\n",
    "\n",
    "# filter weights of the first layer\n",
    "W = layer0['params']['W']\n",
    "print(\"The first layer weights have average magnitude of {:.2}\".format(abs(W).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encourage you to use the below blank code cell to explore the model dictionary! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then iterate over the layers in our model, and load the weights from `trained_vgg` using each layers' `layer.load_weights` method. The final Affine layer is different between our model and the pre-trained model, since the number of classes have changed. Therefore, we break the loop when we encounter the final Affine layer, which has the name `class_layer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_layers = [l for l in model.layers.layers]\n",
    "param_dict_list = trained_vgg['model']['config']['layers']\n",
    "for layer, params in zip(param_layers, param_dict_list):\n",
    "    if(layer.name == 'class_layer'):\n",
    "        break\n",
    "\n",
    "    # To be sure, we print the name of the layer in our model \n",
    "    # and the name in the vgg model.\n",
    "    print(layer.name + \", \" + params['config']['name'])\n",
    "    layer.load_weights(params, load_states=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a check, the above code should have printed out pairs of layer names, from our model and the pre-trained vgg models. The exact name may differ, but the type of layer and layer number should match between the two.\n",
    "\n",
    "## Fine-tuning VGG on the CIFAR-10 dataset\n",
    "Now that we've modified the model for our new CIFAR-10 dataset, and loaded the model weights, let's give training a try!\n",
    "\n",
    "The CIFAR-10 dataset is small enough to fit into memory, meaning that we would normally use an `ArrayIterator` to generate our dataset. However, the CIFAR-10 images are 28x28, and VGG was trained on ImageNet, which as images that are of 224x224 size. For this reason, we use our macrobatching `ImageLoader`, which performs image scaling and cropping on-the-fly. \n",
    "\n",
    "Typically, we would first run `batch_writer.py` to create the macrobatches. With CIFAR-10, we have a special command to do this:\n",
    "\n",
    "```\n",
    "batch_writer.py --set_type cifar10 --data_dir ~/cifar10_macrobatches --macro_size 10000 --target_size 40\n",
    "```\n",
    "\n",
    "This will create macrobatches based on the CIFAR-10 dataset, storing the data in `--data_dir`. Each batch will have `macro_size=10000` images. We have already done this for you, so here we just download the macrobatches and create our `ImageLoader` object.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neon.data import ImageLoader\n",
    "data_dir = 'data/cifar10_macrobatches'\n",
    "\n",
    "img_set_options = dict(repo_dir=data_dir, inner_size=224)\n",
    "train = ImageLoader(set_name='train', scale_range=(256, 384),\n",
    "                    shuffle=True, **img_set_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For fine-tuning, we want the final `Affine` layer to be updated with a higher learning rate compared to the pre-trained weights throughout the rest of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neon.optimizers import GradientDescentMomentum, Schedule, MultiOptimizer\n",
    "from neon.transforms import CrossEntropyMulti\n",
    "# define different optimizers for the class_layer and the rest of the network\n",
    "# we use a momentum coefficient of 0.9 and weight decay of 0.0005.\n",
    "opt_vgg = GradientDescentMomentum(0.001, 0.9, wdecay=0.0005)\n",
    "opt_class_layer = GradientDescentMomentum(0.01, 0.9, wdecay=0.0005)\n",
    "\n",
    "# also define optimizers for the bias layers, which have a different learning rate\n",
    "# and not weight decay.\n",
    "opt_bias = GradientDescentMomentum(0.002, 0.9)\n",
    "opt_bias_class = GradientDescentMomentum(0.02, 0.9)\n",
    "\n",
    "# set up the mapping of layers to optimizers\n",
    "opt = MultiOptimizer({'default': opt_vgg, 'Bias': opt_bias,\n",
    "     'class_layer': opt_class_layer, 'class_layer_bias': opt_bias_class})\n",
    "\n",
    "# use cross-entropy cost to train the network\n",
    "cost = GeneralizedCost(costfunc=CrossEntropyMulti())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we set up callbacks so the model can report progress during training, and then run the `model.fit` function. Note that if you are on a CPU, this next section will take long to finish for you to see results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neon.callbacks.callbacks import Callbacks\n",
    "\n",
    "callbacks = Callbacks(model)\n",
    "model.fit(train, optimizer=opt, num_epochs=10, cost=cost, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the network cost decrease over the course of training as it fine-tunes on this new dataset.\n",
    "\n",
    "## Customize for your own data\n",
    "With neon it is easy to adapt this notebook to fine-tune on your own dataset -- most of the code above can be reused regardless of your specific problem. There just a few things to do:\n",
    "1. Organize your image dataset into a format that our macrobatching loader recognizes. Run our `batch_writer.py` script to generate the macrobatches. See [Macrobatching](http://neon.nervanasys.com/docs/latest/loading_data.html#macrobatching) in our documentation for more information.\n",
    "2. Modify the number of output units in the last `Affine` layer of the network to match the number of categories in your dataset.\n",
    "\n",
    "Feel free to visit our [github](https://github.com/NervanaSystems/neon/) page or our [documention](http://neon.nervanasys.com/docs/latest/index.html) if you have questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
