{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing a custom dataset\n",
    "This notebook will walk you through implementing a custom iterator for a modified version of the Street View House Number (SVHN) dataset. You will then design a network to train on this dataset. \n",
    "\n",
    "## SVHN dataset\n",
    "\n",
    "This dataset is a collection of 73,257 images of house numbers collected from Google Streetview. The original dataset has bounding boxes for all the digits in the image:\n",
    "\n",
    "<img src=\"http://ufldl.stanford.edu/housenumbers/examples_new.png\" width=500px>\n",
    "\n",
    "We have modified the dataset such that each image is 64x64 pixels (with 3 color channels), and the target is a *single* bounding box over all the digits. Your goal is to build a network that, given an image, returns bounding box coordinates for the location of the digit sequence.\n",
    "\n",
    "This notebook is split into two parts:\n",
    "* Writing a custom dataiterator\n",
    "* Building a prediction network\n",
    "\n",
    "## Custom dataset\n",
    "\n",
    "Because the training set of ~27,000 images can fit into the memory of a single Titan X GPU, we could use the `ArrayIterator` class to provide data to the model. However, when the dataset may have more images or larger image sizes, that is no longer an option. Our high-performance `DataLoader`, which loads image in batches and performs complex augmentation, cannot currently handle bounding box data (stay tuned, an object localization dataloader is coming in a future neon release!).\n",
    "\n",
    "We've saved the dataset as a pickle file `svhn_64_box_truncated.p`. This file has a few variables:\n",
    "- `X_train`: a numpy array of shape `(num_examples, num_features)`, where `num_examples = 26624`, and `num_features = 3*64*64 = 12288`\n",
    "- `y_train`: a numpy array of shape `(num_examples, 4)`, with the target bounding box coordinates in `(x_min, y_min, x_max, y_max)` format.\n",
    "\n",
    "Let's first import our backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neon.backends import gen_backend\n",
    "\n",
    "be = gen_backend(batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Below is a skeleton of the SVHN data iterator for you to fill out, with notes to help along the way. The goal is an object that returns, with each call, a tuple of `(X, Y)`, where:\n",
    "- `X`: tensor of shape (num_features, batch_size)\n",
    "- `Y`: tensor of shape (4, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import some useful packages\n",
    "from neon.data import NervanaDataIterator\n",
    "import numpy as np\n",
    "import cPickle\n",
    "import os\n",
    "\n",
    "class SVHN(NervanaDataIterator):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # load data from pickle file\n",
    "        with open('svhn/svhn_64_box_truncated.p') as f:\n",
    "            data = cPickle.load(f)\n",
    "\n",
    "        # generate some random numbers\n",
    "        self.X = data['X_train'] / 255.\n",
    "        self.Y = data['y_train']\n",
    "        \n",
    "        # allocate buffers on the GPU\n",
    "        self.dev_X = self.be.zeros((self.X.shape[1], self.be.bsz))\n",
    "        self.dev_Y = self.be.zeros((self.Y.shape[1], self.be.bsz))\n",
    "\n",
    "        # assign some required attributes\n",
    "        self.ndata = self.X.shape[0]  # number of examples\n",
    "        self.nbatches = self.ndata / self.be.bsz  # number of batches\n",
    "        self.start = 0  # start at zero\n",
    "        self.shape = (3, 64, 64)  # shape of the input\n",
    "\n",
    "    def __iter__(self):\n",
    "        for index in range(self.start, self.ndata, self.be.bsz):\n",
    "            # grab the right slice from the numpy arrays\n",
    "            inputs = self.X[index:(index + self.be.bsz), :]\n",
    "            outputs = self.Y[index:(index + self.be.bsz), :]\n",
    "            inputs = np.ascontiguousarray(inputs.T)\n",
    "            outputs = np.ascontiguousarray(outputs.T)\n",
    "            # transfer to device\n",
    "            self.dev_X.set(inputs)\n",
    "            self.dev_Y.set(outputs)\n",
    "\n",
    "            yield (self.dev_X, self.dev_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your implementation! Below we grab an iteration and print out the output of the dataset. Importantly: make sure that the output tensors are contiguous (e.g. `is_contiguous = True` in the output below). This means that they are allocated on a contiguous set of memory, which is important for the downstream calculations. Contiguity can be broken by operations like transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setup datasets\n",
    "train_set = SVHN(set_name=\"train\")\n",
    "test_set = SVHN(set_name=\"test\")\n",
    "\n",
    "# grab one iteration from the train_set\n",
    "iterator = train_set.__iter__()\n",
    "(X, Y) = iterator.next()\n",
    "print X\n",
    "print Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If all goes well, you are ready to try training on this network! First, let's reset the dataset to zero (since you drew one example from above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVHN.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture\n",
    "To get you started, below we use a toy example that reaches ?? cost after 10 epochs. But you can do better! Play around with adding more layers. \n",
    "\n",
    "If you are feeling ambitious, you can delete the below and try to build a model from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from neon.callbacks.callbacks import Callbacks\n",
    "from neon.initializers import Gaussian\n",
    "from neon.layers import GeneralizedCost, Affine, Conv, Pooling, Linear, Dropout\n",
    "from neon.models import Model\n",
    "from neon.optimizers import GradientDescentMomentum, RMSProp\n",
    "from neon.transforms import Rectlin, Logistic, CrossEntropyMulti, Misclassification, SumSquared\n",
    "\n",
    "init_norm = Gaussian(loc=0.0, scale=0.01)\n",
    "\n",
    "# set up model layers\n",
    "conv = dict(init=init_norm, batch_norm=True, activation=Rectlin())\n",
    "convp1 = dict(init=init_norm, batch_norm=True, activation=Rectlin(), padding=1)\n",
    "\n",
    "layers = [Conv((3, 3, 64), **convp1),  # 64x64 feature map\n",
    "          Conv((3, 3, 64), **convp1),\n",
    "          Pooling((2, 2)),\n",
    "          Dropout(keep=.5),\n",
    "          Conv((3, 3, 96), **convp1),  # 32x32 feature map\n",
    "          Conv((3, 3, 96), **convp1),\n",
    "          Pooling((2, 2)),\n",
    "          Dropout(keep=.5),\n",
    "          Conv((3, 3, 128), **convp1),  # 16x16 feature map\n",
    "          Conv((3, 3, 128), **convp1),\n",
    "          Pooling((2, 2)),\n",
    "          Dropout(keep=.5),\n",
    "          Conv((3, 3, 192), **convp1),  # 8x8 feature map\n",
    "          Conv((1, 1, 192), **conv),\n",
    "          Linear(nout=4, init=init_norm)] # last layer good for bbox\n",
    "\n",
    "# use SumSquared cost\n",
    "cost = GeneralizedCost(costfunc=SumSquared())\n",
    "\n",
    "# setup optimizer\n",
    "optimizer = RMSProp()\n",
    "\n",
    "# initialize model object\n",
    "mlp = Model(layers=layers)\n",
    "\n",
    "# configure callbacks\n",
    "callbacks = Callbacks(mlp)\n",
    "\n",
    "# run fit\n",
    "mlp.fit(train_set, optimizer=optimizer, num_epochs=10, cost=cost, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
